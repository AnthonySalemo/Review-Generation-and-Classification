{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9589e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common unigrams: [('the', 1317375), ('and', 636813), ('a', 636231), ('of', 571970), ('to', 528662)]\n",
      "Most common bigrams: [(('of', 'the'), 152916), (('in', 'the'), 99336), (('this', 'movie'), 60857), (('and', 'the'), 52915), (('is', 'a'), 51707)]\n",
      "Most common trigrams: [(('one', 'of', 'the'), 19310), (('this', 'movie', 'is'), 10226), (('of', 'the', 'film'), 9588), (('this', 'is', 'a'), 9418), (('a', 'lot', 'of'), 9299)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "reviews_df = pd.read_csv('/Users/mazinrafi/Downloads/AllReviews.csv')\n",
    "\n",
    "def simple_tokenizer(text): #\n",
    "    text = str(text)\n",
    "    if text == 'nan':\n",
    "        return [] \n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags. Unlikely needed although a double check wouldn't hurt. \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)  # Filter to allow only alphabet letters\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    # Tokenize by splitting the sentences into words\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the reviews. Replace 'Review' column depending on. \n",
    "reviews_df['Review'] = reviews_df['Review'].astype(str)\n",
    "tokenized_reviews = reviews_df['Review'].apply(simple_tokenizer)\n",
    "flat_token_list = [token for sublist in tokenized_reviews for token in sublist]\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams from the flattened token list\n",
    "unigrams = flat_token_list\n",
    "bigrams = list(ngrams(flat_token_list, 2))\n",
    "trigrams = list(ngrams(flat_token_list, 3))\n",
    "\n",
    "# Count the frequencies of each n-gram. \n",
    "unigram_counts = Counter(unigrams)\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Display the most common n-grams. We expect words like \"the\" and \"a\" to be the most common. \n",
    "print('Most common unigrams:', unigram_counts.most_common(5))\n",
    "print('Most common bigrams:', bigram_counts.most_common(5))\n",
    "print('Most common trigrams:', trigram_counts.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cda410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model generated sentence:\n",
      "lot it of hope of ninja about are someone i\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Function to generate a sentence using the unigram model\n",
    "def generate_sentence_unigram(unigram_counts, num_words=10): #change number accordingly\n",
    "    # Select num_words words based on their frequency probability distribution\n",
    "    words = [word for word in unigram_counts.keys()]\n",
    "    word_probabilities = [unigram_counts[word] for word in words]\n",
    "    generated_words = [random.choices(words, weights=word_probabilities)[0] for _ in range(num_words)]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "\n",
    "\n",
    "#Generate sentences using a unigram model. \n",
    "print(\"Unigram model generated sentence:\")\n",
    "print(generate_sentence_unigram(unigram_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb9eb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great piano these disjointed story initially munho is often put'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counts = defaultdict(Counter)\n",
    "\n",
    "for w1, w2 in bigrams:\n",
    "    bigram_counts[w1][w2] += 1\n",
    "\n",
    "# Convert the counts to probabilities for the bigram model\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    for w2 in bigram_counts[w1]:\n",
    "        bigram_counts[w1][w2] /= total_count\n",
    "\n",
    "def generate_sentence_bigram(bigram_counts, seed_word, num_words=10):\n",
    "    current_word = seed_word\n",
    "    sentence = [current_word]\n",
    "    for _ in range(num_words - 1):  # already have seed word\n",
    "        next_words = list(bigram_counts[current_word].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(bigram_counts[current_word].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        sentence.append(next_word)\n",
    "        current_word = next_word\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Generate a sentence using the bigram model with a seed word\n",
    "seed_word = 'great'  \n",
    "generated_sentence = generate_sentence_bigram(bigram_counts, seed_word)\n",
    "generated_sentence\n",
    "\n",
    "#expected to output different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec4ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model generated sentence starting with bigram 'uncomplicated morality':\n",
      "uncomplicated morality tale starring henry fonda as a rival camp\n"
     ]
    }
   ],
   "source": [
    "trigram_counts = defaultdict(Counter)\n",
    "\n",
    "for w1, w2, w3 in trigrams:\n",
    "    trigram_counts[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = sum(trigram_counts[w1_w2].values())\n",
    "    for w3 in trigram_counts[w1_w2]:\n",
    "        trigram_counts[w1_w2][w3] /= total_count\n",
    "\n",
    "def generate_sentence_trigram(trigram_counts, start_bigram, num_words=10):\n",
    "    if start_bigram not in trigram_counts:\n",
    "        return ' '.join(start_bigram)\n",
    "\n",
    "    current_bigram = start_bigram\n",
    "    sentence = [current_bigram[0], current_bigram[1]]\n",
    "    for _ in range(num_words - 2):  # minus 2 because we already have the start_bigram\n",
    "        next_words = list(trigram_counts[current_bigram].keys())\n",
    "        weights = list(trigram_counts[current_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=weights)[0]\n",
    "        sentence.append(next_word)\n",
    "        current_bigram = (current_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Choose a random start bigram\n",
    "start_bigram = random.choice(list(trigram_counts.keys())) #Alternatively start_bigram=('word1','word2')\n",
    "print(\"Trigram model generated sentence starting with bigram '{} {}':\".format(*start_bigram))\n",
    "print(generate_sentence_trigram(trigram_counts, start_bigram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e9ece0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.13008963881836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pow, log\n",
    "sentence = \"I don't think this movie is that good.\" #Need a longer example to test.\n",
    "test_data = simple_tokenizer(sentence)\n",
    "# Function to calculate perplexity for unigram model\n",
    "def calculate_perplexity_unigram(test_data, unigram_counts, total_unigrams):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in test_data:\n",
    "        N += 1\n",
    "        probability = unigram_counts.get(word, 0) / total_unigrams\n",
    "        if probability > 0:\n",
    "            perplexity = perplexity * (1 / probability)\n",
    "    perplexity = pow(perplexity, 1/float(N))\n",
    "    return perplexity\n",
    "\n",
    "# Total number of unigrams (needed for unigram perplexity calculation)\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Calculating perplexity for unigram model\n",
    "perplexity_unigram = calculate_perplexity_unigram(test_data, unigram_counts, total_unigrams)\n",
    "perplexity_unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7d4786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.086343146031055"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram Perplexity Calculation\n",
    "def calculate_perplexity_bigram(test_data, bigram_counts):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for i in range(len(test_data) - 1):\n",
    "        N += 1\n",
    "        bigram = (test_data[i], test_data[i + 1])\n",
    "        bigram_probability = bigram_counts[test_data[i]].get(test_data[i + 1], 0)\n",
    "        if bigram_probability > 0:\n",
    "            perplexity = perplexity * (1 / bigram_probability)\n",
    "        else:\n",
    "            perplexity = perplexity * (1 / total_unigrams)  # Smoothing for unseen bigrams\n",
    "    perplexity = pow(perplexity, 1/float(N - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Calculating perplexity for bigram model\n",
    "perplexity_bigram = calculate_perplexity_bigram(test_data, bigram_counts)\n",
    "perplexity_bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076c82a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.76777564595017"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate perplexity for trigram model\n",
    "def calculate_perplexity_trigram(test_data, trigram_counts):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for i in range(len(test_data) - 2):\n",
    "        N += 1\n",
    "        trigram = (test_data[i], test_data[i + 1], test_data[i + 2])\n",
    "        trigram_probability = trigram_counts[(test_data[i], test_data[i + 1])].get(test_data[i + 2], 0)\n",
    "        if trigram_probability > 0:\n",
    "            perplexity = perplexity * (1 / trigram_probability)\n",
    "        else:\n",
    "            perplexity = perplexity * (1 / total_unigrams)  # Smoothing for unseen trigrams\n",
    "    perplexity = pow(perplexity, 1/float(N - 2))\n",
    "    return perplexity\n",
    "# Calculating perplexity for trigram model\n",
    "perplexity_trigram = calculate_perplexity_trigram(test_data, trigram_counts)\n",
    "perplexity_trigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a5b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.13008963881836\n",
      "42.086343146031055\n",
      "66.76777564595017\n"
     ]
    }
   ],
   "source": [
    "print(perplexity_unigram)\n",
    "print(perplexity_bigram)\n",
    "print(perplexity_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a4e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        21\n",
      "           1       1.00      0.95      0.97        19\n",
      "\n",
      "    accuracy                           0.97        40\n",
      "   macro avg       0.98      0.97      0.97        40\n",
      "weighted avg       0.98      0.97      0.97        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Unigram Baseline 1: Logistic Regression\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def generate_synthetic_review_unigram(unigram_counts, num_words=50):\n",
    "    words = list(unigram_counts.keys())\n",
    "    word_probabilities = [unigram_counts[word] for word in words]\n",
    "    review = [random.choices(words, weights=word_probabilities)[0] for _ in range(num_words)]\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews = [generate_synthetic_review_unigram(unigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews) + [1] * len(actual_reviews)  # 0 for synthetic, 1 for actual\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using Bag-of-Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_df['Review'])\n",
    "y = data_df['Label']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model1 = LogisticRegression()\n",
    "lr_model1.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5612fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95        21\n",
      "           1       1.00      0.89      0.94        19\n",
      "\n",
      "    accuracy                           0.95        40\n",
      "   macro avg       0.96      0.95      0.95        40\n",
      "weighted avg       0.95      0.95      0.95        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Unigram Baseline 2: Random Forest\n",
    "#Unigram\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model1 = RandomForestClassifier()\n",
    "rf_model1.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e7e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6921 - accuracy: 0.6313 - val_loss: 0.6884 - val_accuracy: 0.9750\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.9625 - val_loss: 0.6814 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 0.9688 - val_loss: 0.6753 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.9688 - val_loss: 0.6698 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6641 - accuracy: 0.9688 - val_loss: 0.6636 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6561 - accuracy: 0.9688 - val_loss: 0.6560 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6466 - accuracy: 0.9688 - val_loss: 0.6476 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6365 - accuracy: 0.9688 - val_loss: 0.6383 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6251 - accuracy: 0.9688 - val_loss: 0.6282 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6125 - accuracy: 0.9688 - val_loss: 0.6174 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4574da4d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Unigram Baseline 3: Feed Forward\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenize and pad the reviews\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(data_df['Review'])\n",
    "sequences = tokenizer.texts_to_sequences(data_df['Review'])\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "fnn_model1 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "fnn_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "fnn_model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd035217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 70ms/step - loss: 0.6867 - accuracy: 0.6000 - val_loss: 0.6767 - val_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6718 - accuracy: 0.8188 - val_loss: 0.6596 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.6503 - accuracy: 0.9375 - val_loss: 0.6304 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.6201 - accuracy: 0.9312 - val_loss: 0.5727 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.5500 - accuracy: 0.9500 - val_loss: 0.4485 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4551 - accuracy: 0.9563 - val_loss: 0.3460 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.3762 - accuracy: 0.9500 - val_loss: 0.2548 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3427 - accuracy: 0.9688 - val_loss: 0.2013 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3184 - accuracy: 0.9688 - val_loss: 0.1904 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2569 - accuracy: 0.9688 - val_loss: 0.1624 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4577cd930>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Unigram Baseline 4: Recurrent Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "lstm_units = 32  # Number of LSTM units\n",
    "\n",
    "# Tokenize and pad the reviews\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(data_df['Review'])\n",
    "sequences = tokenizer.texts_to_sequences(data_df['Review'])\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "rnn_model1 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "rnn_model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f186e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 45:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        23\n",
      "           1       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Unigram Baseline 5: DistilBERT\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizing the dataset\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.encodings = tokenizer(reviews, truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "reviews = data_df['Review'].tolist()\n",
    "labels = data_df['Label'].tolist()\n",
    "dataset = ReviewsDataset(reviews, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DistilBERT model\n",
    "db_model1 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer1 = Trainer(\n",
    "    model=db_model1,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer1.train()\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = trainer1.predict(val_dataset)\n",
    "\n",
    "# Predictions are in the logits format, so convert them to class predictions\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "# Detailed classification report\n",
    "class_report = classification_report(true_labels, preds)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7693e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86        21\n",
      "           1       1.00      0.63      0.77        19\n",
      "\n",
      "    accuracy                           0.82        40\n",
      "   macro avg       0.88      0.82      0.82        40\n",
      "weighted avg       0.87      0.82      0.82        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Bigram Baseline 1: Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def generate_synthetic_review_bigram(bigram_counts, num_words=50):\n",
    "    if not bigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    # Start with a random word\n",
    "    current_word = random.choice(list(bigram_counts.keys()))\n",
    "    review = [current_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        next_words = list(bigram_counts[current_word].keys())\n",
    "        next_word_weights = list(bigram_counts[current_word].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using bigram model\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews_bigram = [generate_synthetic_review_bigram(bigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews_bigram + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews_bigram) + [1] * len(actual_reviews)  # 0 for synthetic, 1 for actual\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "# Using Bag-of-Words model with bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(combined_reviews)\n",
    "y = data_df['Label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model2 = LogisticRegression()\n",
    "lr_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model2.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8fcbb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79        21\n",
      "           1       1.00      0.42      0.59        19\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.83      0.71      0.69        40\n",
      "weighted avg       0.82      0.72      0.70        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Bigram Baseline 2: Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model2 = RandomForestClassifier()\n",
    "rf_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model2.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab0eff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 29ms/step - loss: 0.6963 - accuracy: 0.3438 - val_loss: 0.6935 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.7000 - val_loss: 0.6888 - val_accuracy: 0.9500\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6860 - accuracy: 0.9937 - val_loss: 0.6844 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6810 - accuracy: 0.9875 - val_loss: 0.6801 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.9812 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6714 - accuracy: 0.9812 - val_loss: 0.6718 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6654 - accuracy: 0.9812 - val_loss: 0.6666 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6587 - accuracy: 0.9812 - val_loss: 0.6607 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6507 - accuracy: 0.9875 - val_loss: 0.6539 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6413 - accuracy: 0.9937 - val_loss: 0.6453 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x457493b20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Bigram Baseline 3: Feed Forward\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    bigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        bigrams = [' '.join(tokens[i:i+2]) for i in range(len(tokens) - 1)]\n",
    "        bigram_texts.append(' '.join(bigrams))\n",
    "    return bigram_texts\n",
    "\n",
    "# Apply bigram creation on the dataset\n",
    "bigram_reviews = create_bigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000  # Adjust as needed\n",
    "embedding_dim = 16\n",
    "max_length = 200  # Adjusted for bigrams\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenize and pad the reviews with bigrams\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(bigram_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(bigram_reviews)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "fnn_model2 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "fnn_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "fnn_model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c11d8a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 88ms/step - loss: 0.6891 - accuracy: 0.6562 - val_loss: 0.6834 - val_accuracy: 0.9250\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6795 - accuracy: 0.7750 - val_loss: 0.6737 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6671 - accuracy: 0.8687 - val_loss: 0.6588 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.6534 - accuracy: 0.8938 - val_loss: 0.6371 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.6326 - accuracy: 0.8938 - val_loss: 0.5966 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.5770 - accuracy: 0.9500 - val_loss: 0.5080 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.4668 - accuracy: 0.9500 - val_loss: 0.3661 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.3791 - accuracy: 0.9500 - val_loss: 0.2387 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.2752 - accuracy: 0.9750 - val_loss: 0.1934 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.2187 - accuracy: 0.9688 - val_loss: 0.1813 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x444ac7400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Bigram Baseline 4: Recurrent Neural Network\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    bigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        bigrams = [' '.join(tokens[i:i+2]) for i in range(len(tokens) - 1)]\n",
    "        bigram_texts.append(' '.join(bigrams))\n",
    "    return bigram_texts\n",
    "\n",
    "# Apply bigram creation on the dataset\n",
    "bigram_reviews = create_bigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000  # Adjust as needed\n",
    "embedding_dim = 16\n",
    "max_length = 200  # Adjusted for bigrams\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "lstm_units = 32  # Number of LSTM units\n",
    "\n",
    "# Tokenize and pad the reviews with bigrams\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(bigram_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(bigram_reviews)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "rnn_model2 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "rnn_model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9385e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 4:08:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Bigram Baseline 5: DistilBERT\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "    bigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        bigrams = [' '.join(tokens[i:i+2]) for i in range(len(tokens) - 1)]\n",
    "        bigram_texts.append(' '.join(bigrams))\n",
    "    return bigram_texts\n",
    "\n",
    "# Apply bigram creation on the dataset\n",
    "bigram_reviews = create_bigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizing the dataset\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.encodings = tokenizer(reviews, truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the dataset with bigrams\n",
    "labels = data_df['Label'].tolist()\n",
    "dataset = ReviewsDataset(bigram_reviews, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DistilBERT model\n",
    "db_model2 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer2 = Trainer(\n",
    "    model=db_model2,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer2.train()\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = trainer2.predict(val_dataset)\n",
    "\n",
    "# Predictions are in the logits format, so convert them to class predictions\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "# Detailed classification report\n",
    "class_report = classification_report(true_labels, preds)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9d90659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        21\n",
      "           1       1.00      0.05      0.10        19\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.77      0.53      0.40        40\n",
      "weighted avg       0.76      0.55      0.42        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Trigram Baseline 1: Logistic Regression\n",
    "\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to generate synthetic review using trigrams\n",
    "def generate_synthetic_review_trigram(trigram_counts, num_words=50):\n",
    "    if not trigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    # Start with a random bigram\n",
    "    start_bigram = random.choice(list(trigram_counts.keys()))\n",
    "    review = list(start_bigram)\n",
    "\n",
    "    for _ in range(num_words - 2):\n",
    "        next_words = list(trigram_counts[start_bigram].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(trigram_counts[start_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        start_bigram = (start_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using trigram model\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews_trigram = [generate_synthetic_review_trigram(trigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews_trigram + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews_trigram) + [1] * len(actual_reviews)  # 0 for synthetic, 1 for actual\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews) + [1] * len(actual_reviews)  # 0 for synthetic, 1 for actual\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "# Combine synthetic and actual reviews\n",
    "combined_reviews = synthetic_reviews_trigram + actual_reviews  # Assuming 'actual_reviews' is already defined\n",
    "\n",
    "# Using Bag-of-Words model with trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(combined_reviews)\n",
    "y = data_df['Label']  # Assuming 'data_df' is the DataFrame with labels\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "lr_model3 = LogisticRegression()\n",
    "lr_model3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model3.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c11e0264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        21\n",
      "           1       1.00      0.05      0.10        19\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.77      0.53      0.40        40\n",
      "weighted avg       0.76      0.55      0.42        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Trigram Baseline 2: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to generate synthetic review using trigrams\n",
    "def generate_synthetic_review_trigram(trigram_counts, num_words=50):\n",
    "    if not trigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    start_bigram = random.choice(list(trigram_counts.keys()))\n",
    "    review = list(start_bigram)\n",
    "\n",
    "    for _ in range(num_words - 2):\n",
    "        next_words = list(trigram_counts[start_bigram].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(trigram_counts[start_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        start_bigram = (start_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using trigram model\n",
    "num_synthetic_reviews = 100\n",
    "synthetic_reviews_trigram = [generate_synthetic_review_trigram(trigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "\n",
    "# Combine synthetic and actual reviews\n",
    "combined_reviews = synthetic_reviews_trigram + actual_reviews  # Assuming 'actual_reviews' is already defined\n",
    "\n",
    "# Using Bag-of-Words model with trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(combined_reviews)\n",
    "y = data_df['Label']  # Assuming 'data_df' is the DataFrame with labels\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model3 = RandomForestClassifier()\n",
    "rf_model3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model3.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27f35a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 35ms/step - loss: 0.6896 - accuracy: 0.8813 - val_loss: 0.6858 - val_accuracy: 0.9750\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.9688 - val_loss: 0.6804 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6775 - accuracy: 0.9750 - val_loss: 0.6751 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.6713 - accuracy: 0.9750 - val_loss: 0.6695 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6646 - accuracy: 0.9750 - val_loss: 0.6632 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6570 - accuracy: 0.9750 - val_loss: 0.6563 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6486 - accuracy: 0.9750 - val_loss: 0.6486 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6390 - accuracy: 0.9750 - val_loss: 0.6399 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6283 - accuracy: 0.9750 - val_loss: 0.6302 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6163 - accuracy: 0.9750 - val_loss: 0.6193 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x444a39bd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Trigram Baseline 3: Feed Forward\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create trigrams\n",
    "def create_trigrams(texts):\n",
    "    trigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens) - 2)]\n",
    "        trigram_texts.append(' '.join(trigrams))\n",
    "    return trigram_texts\n",
    "\n",
    "# Apply trigram creation on the dataset\n",
    "trigram_reviews = create_trigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000  # Adjust as needed\n",
    "embedding_dim = 16\n",
    "max_length = 300  # Adjusted for trigrams\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Tokenize and pad the reviews with trigrams\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(trigram_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(trigram_reviews)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "fnn_model3 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "fnn_model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "fnn_model3.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece2fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 2s 101ms/step - loss: 0.6882 - accuracy: 0.6938 - val_loss: 0.6824 - val_accuracy: 0.8250\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 0.6791 - accuracy: 0.7937 - val_loss: 0.6702 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 0.6583 - accuracy: 0.9125 - val_loss: 0.6492 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6443 - accuracy: 0.9312 - val_loss: 0.6146 - val_accuracy: 0.9750\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.6063 - accuracy: 0.9312 - val_loss: 0.5448 - val_accuracy: 0.9750\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.4957 - accuracy: 0.9312 - val_loss: 0.3858 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.3230 - accuracy: 0.9563 - val_loss: 0.2201 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.7706 - accuracy: 0.5938 - val_loss: 0.1951 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.3060 - accuracy: 0.9500 - val_loss: 0.2217 - val_accuracy: 0.9750\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 0.2987 - accuracy: 0.9500 - val_loss: 0.2195 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x4461109d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####Trigram Baseline 4: Recurrent Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create trigrams\n",
    "def create_trigrams(texts):\n",
    "    trigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens) - 2)]\n",
    "        trigram_texts.append(' '.join(trigrams))\n",
    "    return trigram_texts\n",
    "\n",
    "# Apply trigram creation on the dataset\n",
    "trigram_reviews = create_trigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Define parameters\n",
    "vocab_size = 10000  # Adjust as needed\n",
    "embedding_dim = 16\n",
    "max_length = 300  # Adjusted for trigrams\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "lstm_units = 32  # Number of LSTM units\n",
    "\n",
    "# Tokenize and pad the reviews with trigrams\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(trigram_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(trigram_reviews)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Prepare the labels\n",
    "labels = data_df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "rnn_model3 = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    LSTM(lstm_units),\n",
    "    Dense(24, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # Adjust as needed\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "rnn_model3.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71adfd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 7:53:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Trigram Baseline 5: DistilBERT\n",
    "\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# Function to create trigrams\n",
    "def create_trigrams(texts):\n",
    "    trigram_texts = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        trigrams = [' '.join(tokens[i:i+3]) for i in range(len(tokens) - 2)]\n",
    "        trigram_texts.append(' '.join(trigrams))\n",
    "    return trigram_texts\n",
    "\n",
    "# Apply trigram creation on the dataset\n",
    "trigram_reviews = create_trigrams(data_df['Review'].tolist())\n",
    "\n",
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenizing the dataset\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.encodings = tokenizer(reviews, truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the dataset with trigrams\n",
    "labels = data_df['Label'].tolist()\n",
    "dataset = ReviewsDataset(trigram_reviews, labels)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DistilBERT model\n",
    "db_model3 = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer3 = Trainer(\n",
    "    model=db_model3,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer3.train()\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = trainer3.predict(val_dataset)\n",
    "\n",
    "# Predictions are in the logits format, so convert them to class predictions\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "# Detailed classification report\n",
    "class_report = classification_report(true_labels, preds)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
