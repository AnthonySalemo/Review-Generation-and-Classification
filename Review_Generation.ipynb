{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOm1usR3SWLk",
        "outputId": "f6af8fbb-8304-4e21-d969-f8a9fb47b4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.util import ngrams\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfahMbePSaiQ",
        "outputId": "d368aacf-9806-4115-dc55-5b463b8da7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import GPT2's tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('.')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "GPT2model = GPT2LMHeadModel.from_pretrained('.')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GPT2model.to(device)\n",
        "GPT2model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmZ6hKL9Sbsi",
        "outputId": "5d6e35ae-275c-4494-b722-77ab1a4b740c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test out model and tokenizer with a prompting example\n",
        "sequence = \"write a movie review:\"\n",
        "\n",
        "inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
        "attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "attention_mask = attention_mask.to(device)"
      ],
      "metadata": {
        "id": "fCYeNzYtSd9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gentext = []\n",
        "\n",
        "for i in range(5000):\n",
        "  with torch.no_grad():\n",
        "    outputs = GPT2model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,  # Add the attention mask here\n",
        "        do_sample=True,\n",
        "        top_k=100,\n",
        "        pad_token_id=tokenizer.pad_token_id,  # Explicitly set the pad token ID\n",
        "        max_new_tokens = 200,\n",
        "        min_new_tokens = 100,\n",
        "        temperature = 0.7,\n",
        "        top_p = 0.95,\n",
        "        no_repeat_ngram_size = 3\n",
        "        )\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  generated_text.rsplit('.', 1)[0] + '.'\n",
        "  gentext.append(generated_text)\n",
        "  if i % 100 == 0:\n",
        "    print(i, '/', 5000)\n",
        "\n",
        "#gentext_df = pd.DataFrame(gentext, columns = ['review'])\n",
        "#gentext_df['prompt'] = sequence\n",
        "#gentext_df['model'] = 'distilgpt2'\n",
        "gentext_df = pd.DataFrame({'model': 'distilgpt2_finetuned', 'prompt': sequence, 'review': gentext})\n",
        "\n",
        "gentext_df.to_csv('distilgpt_finetuned_reviews.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74GshOchSk-K",
        "outputId": "592db2c4-b978-4df4-b4e7-94823932eeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 5000\n",
            "100 / 5000\n",
            "200 / 5000\n",
            "300 / 5000\n",
            "400 / 5000\n",
            "500 / 5000\n",
            "600 / 5000\n",
            "700 / 5000\n",
            "800 / 5000\n",
            "900 / 5000\n",
            "1000 / 5000\n",
            "1100 / 5000\n",
            "1200 / 5000\n",
            "1300 / 5000\n",
            "1400 / 5000\n",
            "1500 / 5000\n",
            "1600 / 5000\n",
            "1700 / 5000\n",
            "1800 / 5000\n",
            "1900 / 5000\n",
            "2000 / 5000\n",
            "2100 / 5000\n",
            "2200 / 5000\n",
            "2300 / 5000\n",
            "2400 / 5000\n",
            "2500 / 5000\n",
            "2600 / 5000\n",
            "2700 / 5000\n",
            "2800 / 5000\n",
            "2900 / 5000\n",
            "3000 / 5000\n",
            "3100 / 5000\n",
            "3200 / 5000\n",
            "3300 / 5000\n",
            "3400 / 5000\n",
            "3500 / 5000\n",
            "3600 / 5000\n",
            "3700 / 5000\n",
            "3800 / 5000\n",
            "3900 / 5000\n",
            "4000 / 5000\n",
            "4100 / 5000\n",
            "4200 / 5000\n",
            "4300 / 5000\n",
            "4400 / 5000\n",
            "4500 / 5000\n",
            "4600 / 5000\n",
            "4700 / 5000\n",
            "4800 / 5000\n",
            "4900 / 5000\n"
          ]
        }
      ]
    }
  ]
}