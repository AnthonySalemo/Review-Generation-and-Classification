{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.util import ngrams\n",
    "\n",
    "reviews_df = pd.read_csv('/Users/mazinrafi/Downloads/AllReviews.csv')\n",
    "\n",
    "def simple_tokenizer(text): #\n",
    "    text = str(text)\n",
    "    if text == 'nan':\n",
    "        return [] \n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags. Unlikely needed although a double check wouldn't hurt. \n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)  # Filter to allow only alphabet letters\n",
    "    text = text.lower()  # Convert to lower case\n",
    "    # Tokenize by splitting the sentences into words\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize the reviews. Replace 'Review' column depending on. \n",
    "reviews_df['Review'] = reviews_df['Review'].astype(str)\n",
    "tokenized_reviews = reviews_df['Review'].apply(simple_tokenizer)\n",
    "flat_token_list = [token for sublist in tokenized_reviews for token in sublist]\n",
    "\n",
    "# Generate unigrams, bigrams, and trigrams from the flattened token list\n",
    "unigrams = flat_token_list\n",
    "bigrams = list(ngrams(flat_token_list, 2))\n",
    "trigrams = list(ngrams(flat_token_list, 3))\n",
    "\n",
    "# Count the frequencies of each n-gram. \n",
    "unigram_counts = Counter(unigrams)\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Display the most common n-grams. We expect words like \"the\" and \"a\" to be the most common. \n",
    "print('Most common unigrams:', unigram_counts.most_common(5))\n",
    "print('Most common bigrams:', bigram_counts.most_common(5))\n",
    "print('Most common trigrams:', trigram_counts.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cda410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model generated sentence:\n",
      "me after a by dont go the was of taking\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Function to generate a sentence using the unigram model\n",
    "def generate_sentence_unigram(unigram_counts, num_words=10): #change number accordingly\n",
    "    # Select num_words words based on their frequency probability distribution\n",
    "    words = [word for word in unigram_counts.keys()]\n",
    "    word_probabilities = [unigram_counts[word] for word in words]\n",
    "    generated_words = [random.choices(words, weights=word_probabilities)[0] for _ in range(num_words)]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "\n",
    "\n",
    "#Generate sentences using a unigram model. \n",
    "print(\"Unigram model generated sentence:\")\n",
    "print(generate_sentence_unigram(unigram_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb9eb43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great outdoors beautifully slinky figure out of course not empty'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counts = defaultdict(Counter)\n",
    "\n",
    "for w1, w2 in bigrams:\n",
    "    bigram_counts[w1][w2] += 1\n",
    "\n",
    "# Convert the counts to probabilities for the bigram model\n",
    "for w1 in bigram_counts:\n",
    "    total_count = float(sum(bigram_counts[w1].values()))\n",
    "    for w2 in bigram_counts[w1]:\n",
    "        bigram_counts[w1][w2] /= total_count\n",
    "\n",
    "def generate_sentence_bigram(bigram_counts, seed_word, num_words=10):\n",
    "    current_word = seed_word\n",
    "    sentence = [current_word]\n",
    "    for _ in range(num_words - 1):  # already have seed word\n",
    "        next_words = list(bigram_counts[current_word].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(bigram_counts[current_word].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        sentence.append(next_word)\n",
    "        current_word = next_word\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Generate a sentence using the bigram model with a seed word\n",
    "seed_word = 'great'  \n",
    "generated_sentence = generate_sentence_bigram(bigram_counts, seed_word)\n",
    "generated_sentence\n",
    "\n",
    "#expected to output different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec4ec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model generated sentence starting with bigram 'bands early':\n",
      "bands early stuff and is there a single revision obviously\n"
     ]
    }
   ],
   "source": [
    "trigram_counts = defaultdict(Counter)\n",
    "\n",
    "for w1, w2, w3 in trigrams:\n",
    "    trigram_counts[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in trigram_counts:\n",
    "    total_count = sum(trigram_counts[w1_w2].values())\n",
    "    for w3 in trigram_counts[w1_w2]:\n",
    "        trigram_counts[w1_w2][w3] /= total_count\n",
    "\n",
    "def generate_sentence_trigram(trigram_counts, start_bigram, num_words=10):\n",
    "    if start_bigram not in trigram_counts:\n",
    "        return ' '.join(start_bigram)\n",
    "\n",
    "    current_bigram = start_bigram\n",
    "    sentence = [current_bigram[0], current_bigram[1]]\n",
    "    for _ in range(num_words - 2):  # minus 2 because we already have the start_bigram\n",
    "        next_words = list(trigram_counts[current_bigram].keys())\n",
    "        weights = list(trigram_counts[current_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=weights)[0]\n",
    "        sentence.append(next_word)\n",
    "        current_bigram = (current_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Choose a random start bigram\n",
    "start_bigram = random.choice(list(trigram_counts.keys())) #Alternatively start_bigram=('word1','word2')\n",
    "print(\"Trigram model generated sentence starting with bigram '{} {}':\".format(*start_bigram))\n",
    "print(generate_sentence_trigram(trigram_counts, start_bigram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e9ece0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.13008963881836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import pow, log\n",
    "sentence = \"I don't think this movie is that good.\" #Need a longer example to test.\n",
    "test_data = simple_tokenizer(sentence)\n",
    "# Function to calculate perplexity for unigram model\n",
    "def calculate_perplexity_unigram(test_data, unigram_counts, total_unigrams):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for word in test_data:\n",
    "        N += 1\n",
    "        probability = unigram_counts.get(word, 0) / total_unigrams\n",
    "        if probability > 0:\n",
    "            perplexity = perplexity * (1 / probability)\n",
    "    perplexity = pow(perplexity, 1/float(N))\n",
    "    return perplexity\n",
    "\n",
    "# Total number of unigrams (needed for unigram perplexity calculation)\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Calculating perplexity for unigram model\n",
    "perplexity_unigram = calculate_perplexity_unigram(test_data, unigram_counts, total_unigrams)\n",
    "perplexity_unigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7d4786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.086343146031055"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram Perplexity Calculation\n",
    "def calculate_perplexity_bigram(test_data, bigram_counts):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for i in range(len(test_data) - 1):\n",
    "        N += 1\n",
    "        bigram = (test_data[i], test_data[i + 1])\n",
    "        bigram_probability = bigram_counts[test_data[i]].get(test_data[i + 1], 0)\n",
    "        if bigram_probability > 0:\n",
    "            perplexity = perplexity * (1 / bigram_probability)\n",
    "        else:\n",
    "            perplexity = perplexity * (1 / total_unigrams)  # Smoothing for unseen bigrams\n",
    "    perplexity = pow(perplexity, 1/float(N - 1))\n",
    "    return perplexity\n",
    "\n",
    "# Calculating perplexity for bigram model\n",
    "perplexity_bigram = calculate_perplexity_bigram(test_data, bigram_counts)\n",
    "perplexity_bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076c82a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.76777564595017"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate perplexity for trigram model\n",
    "def calculate_perplexity_trigram(test_data, trigram_counts):\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for i in range(len(test_data) - 2):\n",
    "        N += 1\n",
    "        trigram = (test_data[i], test_data[i + 1], test_data[i + 2])\n",
    "        trigram_probability = trigram_counts[(test_data[i], test_data[i + 1])].get(test_data[i + 2], 0)\n",
    "        if trigram_probability > 0:\n",
    "            perplexity = perplexity * (1 / trigram_probability)\n",
    "        else:\n",
    "            perplexity = perplexity * (1 / total_unigrams)  # Smoothing for unseen trigrams\n",
    "    perplexity = pow(perplexity, 1/float(N - 2))\n",
    "    return perplexity\n",
    "# Calculating perplexity for trigram model\n",
    "perplexity_trigram = calculate_perplexity_trigram(test_data, trigram_counts)\n",
    "perplexity_trigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a5b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171.13008963881836\n",
      "42.086343146031055\n",
      "66.76777564595017\n"
     ]
    }
   ],
   "source": [
    "print(perplexity_unigram)\n",
    "print(perplexity_bigram)\n",
    "print(perplexity_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61a4e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_synthetic_review_unigram(unigram_counts, num_words=50):\n",
    "    words = list(unigram_counts.keys())\n",
    "    word_probabilities = [unigram_counts[word] for word in words]\n",
    "    review = [random.choices(words, weights=word_probabilities)[0] for _ in range(num_words)]\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews = [generate_synthetic_review_unigram(unigram_counts) for _ in range(num_synthetic_reviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53429608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming reviews_df is your DataFrame containing actual reviews\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7f6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews) + [1] * len(actual_reviews)  # 0 for synthetic, 1 for actual\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57dc8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using Bag-of-Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_df['Review'])\n",
    "y = data_df['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fb1fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model1 = LogisticRegression()\n",
    "lr_model1.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a585cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        21\n",
      "           1       1.00      0.74      0.85        19\n",
      "\n",
      "    accuracy                           0.88        40\n",
      "   macro avg       0.90      0.87      0.87        40\n",
      "weighted avg       0.90      0.88      0.87        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions = lr_model1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b1bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: and the out he one that even you in just was as of read such role seen michael check dr audience find plain lots acting each entitled brief sleaze guests rock ruth gift wants inner benefited ferrell joshs daring nunez glorified ripples remember user\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: and this the it to is old could for on was be am much couldnt story part an chupacabra killer get editor another anywhere showing stella evaluate whats want disney wild right directed tension harry becomes\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: this the all its to one think really that with in but by was of worst his about what did using most an ending including line depth main acting wacko horrible unusual brian casablanca screenwriters became latifah white remotely recycled forgetting\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: and this the all only to one these for but as of camera foul unbelievable while over forthright seen part sure accusations own uses antihero speak yourself entertainment exchange homage nbc need view most womanshaving civil banal rachel exit officer blair use trebor\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: and this the it only he last your is have you never could for in but when they by film movies as of actors him times some which black favor directed back whole acting concoction supposed depravity mids anything live sadism\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random indices for selecting sample reviews\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "\n",
    "# Extract sample reviews and their actual labels\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "\n",
    "# Predict the labels for these reviews\n",
    "predicted_labels = lr_model1.predict(sample_reviews)\n",
    "\n",
    "# Display the results\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "\n",
    "    # Transforming the review features back to text (may not perfectly reconstruct the original text)\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    \n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7693e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_review_bigram(bigram_counts, num_words=50):\n",
    "    if not bigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    # Start with a random word\n",
    "    current_word = random.choice(list(bigram_counts.keys()))\n",
    "    review = [current_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        next_words = list(bigram_counts[current_word].keys())\n",
    "        next_word_weights = list(bigram_counts[current_word].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using bigram model\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews_bigram = [generate_synthetic_review_bigram(bigram_counts) for _ in range(num_synthetic_reviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d50208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86        21\n",
      "           1       1.00      0.63      0.77        19\n",
      "\n",
      "    accuracy                           0.82        40\n",
      "   macro avg       0.88      0.82      0.82        40\n",
      "weighted avg       0.87      0.82      0.82        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using Bag-of-Words model with bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(combined_reviews)\n",
    "y = data_df['Label']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model2 = LogisticRegression()\n",
    "lr_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model2.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8fcbb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: is the and the it and the best the story for the of the that it the acting from the has the the special br br is just this film if you they will don put film has and don the rest rest of read the special effects the actors which is the original really bad through the not be the novel but that will not are decent save your and john bad if you read the books books do do yourself yourself favor favor and put yourself yourself through the agony agony of of sitting sitting through through this this travesty travesty the story line line which which skips skips about about 70 70 of original story line wanders wanders miles miles from novel except except for for rachel rachel weisz weisz great great voice voice over over work work for the dragon dragon which best acting acting this acting from actors is just above above high high school school play play level level what what were were jeremy jeremy and john thinking thinking in in taking taking these these roles roles the effects are decent but the sets sets are are cheesy cheesy save your cash cash and and wait wait for the third third novel novel br br let let hope hope they be sequel\n",
      "Actual Label: Actual\n",
      "Predicted Label: Actual\n",
      "--------------------------------------------------\n",
      "Review: this the to the one the and the and that on the the thing but is it on in the to make do and on and and right sort of his the of the it would and now this would she is of this this is have the all the there and and some being the at the and all but in the acting acting is of his and they the people this time just so the time in he find out in this the same he is any of the end and interesting to be should be back to to see br br her and great and to find lack of understand why what he he did begin to him in the script like the his daughter the father to do would be try to looking for was very but it you don don like into the of an it like the girl and not to write anyone who boring and for my script is going to build on up to he was is very not in why this is happening they are what she but finally kills the with them them in the head the baby baby and picks her her up after the she was be she later on that she didn have and don they find the women put together it isn don know of time elements of is boring it has the moment the wrong but you ultimately fail time to they don all this for john both of good and they take though it why it he would he gets is well add up movie but went into into video video store store and and looked looked around around to find some some horror horror movies movies after after about about 30 30 minutes minutes just just rushed rushed and and picked picked out out few few stumbled stumbled upon upon masters masters of of horror horror which which contained contained pro pro life life and right to to die die they they seemed seemed ok ok same same old old cheesy cheesy horror horror crap crap but but was was interested interested for for some some reason reason it it said said about about pro life on the case case about about being being classic classic return return to to form form for john carpenter carpenter loved loved his thing so so thought thought this be good all that that so so turned turned it on thinking thinking it be something something great interesting was very wrong wrong it it started started off off casual casual just just girl girl running running through through forest forest scared scared of of something something car car stops stops and and picks up just so being people she she needed needed to see amazing amazing they take her her back to some some clinic clinic and and examine examine her her at the sametime sametime all happening her her father father appears appears at the gates gates and don allow allow him he isn isn aloud aloud near near the the area area most most likely likely from from something something he would of of done done in the past past but know of of any this at moment he he really really does does not not want want his daughter in this place place an an abortion abortion center center he very strongly strongly against against such such acts acts believing believing it it sickening sickening and not what what god god would would want want he he supports supports what what heard heard is is called called pro life acting acting against against abortions abortions and and going to extremes extremes to to allow allow the the babies babies to be born born they are sick sick they the life life of an unborn unborn being being taken taken yet yet they they ve ve killed killed humans humans in past to the birth birth justice justice is is only only figment figment of the mind mind anyway anyway back back on on track track after girl is is examined examined they out shes shes pregnant pregnant but but far far ahead ahead than than what she should only few few weeks weeks pregnant is months months ahead ahead she she keeps keeps telling telling them them they they wont wont understand understand her she wants wants an abortion and all but finally tells tells the the truth truth that was raped raped by by demon demon from from hell hell and that her father wants wants this this baby baby but but believes believes god god wants baby not not who who truly truly does does he gets his his sons sons they they arm arm themselves themselves with with pistols pistols and and shotguns shotguns and and begin make they they re re way way into the clinic clinic shooting shooting down down anyone who won won co co operate operate the head of clinic who who must must of of had had trouble trouble with past is well prepared prepared this time ends ends up up killing killing one the fathers fathers sons sons but end gets gets shot shot few few times times wearing wearing bullet bullet proof proof jacket jacket the father then then performs performs what he believes believes is is done done to women he he cuts cuts hole hole where where the the vagina vagina would be if if he he we we re re female female and and sticks sticks some some sort of sucking sucking thing thing up up there and sucks sucks out out all this blood blood whilst whilst all happening the girl gives gives birth birth to some demonic demonic baby baby with with many many legs legs and some demon demon raises raises from from beneath beneath the the earth earth not same room room and and starts starts looking for its its child child the father sees sees this this later starts questioning questioning why this happened happened he did what was told told to and doesn doesn understand like this the demon demon had had killed killed both sons earlier earlier and now goes goes for for father father whilst whilst the girl kills demon carries carries it it away away not same scene scene br br yeah yeah it it probably probably sounds sounds pretty pretty cool cool and and thrilling thrilling horror horror movie isn the is horrible horrible and and lacks lacks enthusiasm enthusiasm the not even even creative creative they they choose choose the wrong characters characters and don even even build on them them just just everything everything put together all the small small parts parts don even add to something great waste waste of time wouldn wouldn classify classify this this as as horror horror though has elements horror they they ultimately fail at at what what they they try to succeed succeed it it felt felt more more like like beginners beginners short short movie movie than than by by john carpenter br br sorry sorry for my lack of information information and and detailed detailed review review just just didn to waste waste to write something something exciting exciting also also sorry sorry if if my my spelling spelling and and details details are are incorrect incorrect couldn couldn really really be be bothered bothered to to research research anything\n",
      "Actual Label: Actual\n",
      "Predicted Label: Actual\n",
      "--------------------------------------------------\n",
      "Review: the the the it is and it is and of upon the the chaotic chaotic golden golden sure sure well well driftwood driftwood of of harp harp mess mess hollywood hollywood very very drives drives might might the the whos whos irritating irritating list list or or can can water water even even can can on on dont dont before before this this is of all all it it pretentious pretentious so so polar polar spacek spacek in in lady lady worked worked it it every every be be talk talk work work clear clear main main the\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: and the movie that the movie movie was was the it is was not and there of the and movie this is movie and over and and think that is it to seen that the war to see like it it was this film some of of these the plot if you war is is over there are are still still some some heroes heroes left left few few of these former former military military pilots pilots get get together together to to deliver deliver war war surplus surplus aircraft aircraft to to their their new new mostly mostly civilian civilian owners owners and and these these deliveries deliveries take take them them around around the the world world as as wanna wanna be be or or wish wish could could have have been been combat combat pilot pilot got got to see some the true true aircraft aircraft stars stars of of wwii wwii even even the the soap soap opera opera romance romance scenes scenes were were kinda kinda cute cute and and light light hearted hearted actually actually enjoyed enjoyed the think it was very very entertaining entertaining the the stars stars interacted interacted well well and not too too unbalanced unbalanced was plot familiar familiar and the ending ending predictable predictable absolutely absolutely so so what what name name single single elvis elvis presley presley or or james james bond bond movie that wasn wasn as as well well this film was not academy academy award award material material but but it is much much better better than than several several films films have have seen that have have won won academy academy awards awards it is also also much than most most of the trash trash that is being being produced produced for for tv tv and movie theaters theaters today today for for one one hope hope it it makes makes it to dvd dvd if you don don like it heck heck this is america america don don watch watch it\n",
      "Actual Label: Actual\n",
      "Predicted Label: Actual\n",
      "--------------------------------------------------\n",
      "Review: that the the of really to be movie in or the like like there there full full movie movie who who good good in in work work harem harem about about have have its its romance romance the the really to puzzle puzzle this this effects effects ideas ideas used used he he bit bit effort effort because because evenly evenly in or impressive impressive this this other other that that holding holding the of everyone everyone of of stars stars care care especially especially lets lets be movie listed listed gives gives my my years\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display a few classified reviews (adapted for bigram features)\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "predicted_labels = lr_model2.predict(sample_reviews)\n",
    "\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9d90659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        21\n",
      "           1       1.00      0.05      0.10        19\n",
      "\n",
      "    accuracy                           0.55        40\n",
      "   macro avg       0.77      0.53      0.40        40\n",
      "weighted avg       0.76      0.55      0.42        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic_review_trigram(trigram_counts, num_words=50):\n",
    "    if not trigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    # Start with a random bigram\n",
    "    start_bigram = random.choice(list(trigram_counts.keys()))\n",
    "    review = list(start_bigram)\n",
    "\n",
    "    for _ in range(num_words - 2):\n",
    "        next_words = list(trigram_counts[start_bigram].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(trigram_counts[start_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        start_bigram = (start_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using trigram model\n",
    "num_synthetic_reviews = 100  # adjust as needed\n",
    "synthetic_reviews_trigram = [generate_synthetic_review_trigram(trigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using Bag-of-Words model with trigrams (can change later)\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(combined_reviews)\n",
    "y = data_df['Label']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lr_model3 = LogisticRegression()\n",
    "lr_model3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = lr_model3.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cba298a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: enough was she was she an she an to an to also to also feel also feel his feel his may his may key may key resnais key resnais in resnais in who in who its who its with its with progress with progress in progress in by in by but by but movie but movie of movie of tears of tears sister tears sister very sister very movie very movie in movie in adults in adults had adults had have had have who have who distorted who distorted exchange distorted exchange skills exchange skills film skills film chance film chance if chance if to if to it to it apparently it apparently well apparently well the well the and the and great and great look great look fine look fine an fine an tv an tv nice tv nice party\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: this chupacabra the chupacabra the killer the killer to killer to the to the get the get the get the am the am editor am editor old editor old was old was another was another anywhere another anywhere showing anywhere showing stella showing stella evaluate stella evaluate to evaluate to to to to whats to whats an whats an be an be it be it the it the story the story couldnt story couldnt part couldnt part to part to for to for the for the on the on want on want is want is on is on much on much disney much disney wild disney wild it wild it and it and the and the could the could and could and right and right directed right directed tension directed tension is tension is harry is harry becomes\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: br br the don watch it off br br this movie was br br if about as much as much as from this movie the only excitement only excitement got excitement got from got from this movie was from was from watching from watching the watching the rather the rather pert rather pert behind pert behind of behind of tamara of tamara sexy tamara sexy eyes sexy eyes davies eyes davies who davies who has who has clearly has clearly been clearly been doing been doing some doing some yoga some yoga br yoga br br br the whole the whole an whole an ex an ex cia ex cia agent cia agent whose agent whose still whose still really still really working really working for working for the for the cia the cia shhhh cia shhhh don shhhh don tell don tell anyone tell anyone got anyone got old got old after old after mission after mission impossible mission impossible and impossible and stinks and stinks here stinks here about here about as much as mi3 as mi3 does mi3 does br does br br br br lovely br lovely tamara lovely tamara runs tamara runs around runs around lot around lot which lot which is which is good is good really good really good really good pause good pause rewind pause rewind pause rewind pause rewind pause rewind but rewind but the but the plot the plot storyline plot storyline and storyline and ending and ending are ending are just are just rip just rip after rip after rip after rip after after rip off rip off br br if your if your seagal your seagal fan seagal fan don fan don watch watch it he it he was he was probably was probably depressed probably depressed when depressed when he when he made he made it made it and it and now and now am\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: he than but than but is but is that is that of that of maybe of maybe on maybe on things on things have things have against have against so against so though so though couldnt though couldnt there couldnt there far there far fabulous far fabulous readying fabulous readying disturbed readying disturbed and disturbed and verbally and verbally this verbally this lady this lady are lady are point are point hanoi point hanoi ever hanoi ever out ever out the out the previous the previous in previous in to in to gone to gone adults gone adults off adults off rich off rich granny rich granny we granny we communicating we communicating cena communicating cena of cena of is of is his is his story his story every story every the every the ventriloquists\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: the acting is in the end in the same the script is them in the picks her up to do and but in the you don know boring and not went into video into video store video store and store and looked and looked around looked around to around to find to find some find some horror some horror movies horror movies after movies after about after about 30 about 30 minutes 30 minutes just minutes just rushed just rushed and rushed and picked and picked out picked out few out few stumbled few stumbled upon stumbled upon masters upon masters of masters of horror of horror which horror which contained which contained pro contained pro life pro life and life and right and right to right to die to die they die they seemed they seemed ok seemed ok same ok same old same old cheesy old cheesy horror cheesy horror crap horror crap but crap but was but was interested was interested for interested for some for some reason some reason it reason it said it said about said about pro about pro life pro life on life on the on the case the case about case about being about being classic being classic return classic return to return to form to form for form for john for john carpenter john carpenter loved carpenter loved his loved his the his the thing the thing so thing so thought so thought this thought this would this would be would be good be good and good and all and all that all that so that so turned so turned it turned it on it on thinking on thinking it thinking it would it would be would be something be something great something great and great and interesting and interesting was interesting was very was very wrong very wrong it wrong it started it started off started off casual off casual just casual just girl just girl running girl running through running through forest through forest scared forest scared of scared of something of something car something car stops car stops and stops and picks and picks her her up just up just so just so being so being the being the people the people she people she needed she needed to needed to see to see amazing see amazing they amazing they take they take her take her back her back to back to some to some clinic some clinic and clinic and examine and examine her examine her at her at the at the sametime the sametime all sametime all this all this is this is happening is happening her happening her father her father appears father appears at appears at the at the gates the gates and gates and they and they don they don allow don allow him allow him in him in he in he isn he isn aloud isn aloud near aloud near the near the area the area most area most likely most likely from likely from something from something he something he would he would of would of done of done in done in the in the past the past but past but you but you don don know of know of any of any of any of this of this at this at the at the moment the moment he moment he really he really does really does not does not want not want his want his daughter his daughter in daughter in this in this place this place an place an abortion an abortion center abortion center he center he is he is very is very strongly very strongly against strongly against such against such acts such acts believing acts believing it believing it sickening it sickening and sickening and not and not what not what god what god would god would want would want he want he supports he supports what supports what heard what heard is heard is called is called pro called pro life pro life acting life acting against acting against abortions against abortions and abortions and going and going to going to extremes to extremes to extremes to allow to allow the allow the babies the babies to babies to be to be born be born they born they are they are sick are sick they sick they don they don like don like the like the life the life of life of an of an unborn an unborn being unborn being taken being taken yet taken yet they yet they ve they ve killed ve killed humans killed humans in humans in the the past to past to allow allow the birth the birth justice birth justice is justice is only is only figment only figment of figment of the of the mind the mind anyway mind anyway back anyway back on back on track on track after track after the after the girl the girl is girl is examined is examined they examined they find they find out find out shes out shes pregnant shes pregnant but pregnant but far but far ahead far ahead than ahead than what than what she what she should she should be should be she be she is she is only is only few only few weeks few weeks pregnant weeks pregnant but pregnant but is but is months is months ahead months ahead she ahead she keeps she keeps telling keeps telling them telling them they them they wont they wont understand wont understand her understand her and her and that and that she that she wants she wants an wants an abortion an abortion and abortion and all and all but all but finally but finally tells finally tells the tells the truth the truth that truth that she that she was she was raped was raped by raped by demon by demon from demon from hell from hell and hell and that and that her that her father her father wants father wants this wants this baby this baby but baby but believes but believes god believes god wants god wants this this baby not baby not who not who truly who truly does truly does he does he gets he gets his gets his sons his sons they sons they arm they arm themselves arm themselves with themselves with pistols with pistols and pistols and shotguns and shotguns and shotguns and begin and begin to begin to make to make they make they re they re way re way into way into the into the clinic the clinic shooting clinic shooting down shooting down anyone down anyone who anyone who won who won co won co operate co operate the operate the head the head of head of the of the clinic the clinic who clinic who must who must of must of had of had trouble had trouble with trouble with them with them in the past is past is well is well prepared well prepared this prepared this time this time ends time ends up ends up killing up killing one killing one the one the fathers the fathers sons fathers sons but sons but in the end gets end gets shot gets shot few shot few times few times wearing times wearing bullet wearing bullet proof bullet proof jacket proof jacket the jacket the father the father then father then performs then performs what performs what he what he believes he believes is believes is done is done to done to the to the women the women he women he cuts he cuts hole cuts hole where hole where the where the vagina the vagina would vagina would be would be if be if he if he we he we re we re female re female and female and sticks and sticks some sticks some sort some sort of sort of sucking of sucking thing sucking thing up thing up there up there and there and sucks and sucks out sucks out all out all this all this blood this blood whilst blood whilst all whilst all this is happening the happening the girl the girl gives girl gives birth gives birth to birth to some to some demonic some demonic baby demonic baby with baby with many with many legs many legs and legs and some and some demon some demon raises demon raises from raises from beneath from beneath the beneath the earth the earth not earth not in not in the the same room same room and room and starts and starts looking starts looking for looking for its for its child its child the child the father the father sees father sees this sees this later this later on later on and on and starts and starts questioning starts questioning why questioning why this why this happened this happened he happened he did he did what did what he what he was he was told was told to told to do do and doesn and doesn understand doesn understand why understand why it why it like it like this like this the this the demon the demon had demon had killed had killed both killed both of both of his of his sons his sons earlier sons earlier and earlier and now and now goes now goes for goes for father for father whilst father whilst the whilst the girl the girl kills girl kills the kills the baby the baby and baby and the and the demon the demon carries demon carries it carries it away it away not away not in the same scene same scene br scene br br br br yeah br yeah it yeah it probably it probably sounds probably sounds pretty sounds pretty cool pretty cool and cool and thrilling and thrilling horror thrilling horror movie horror movie but movie but it but it isn it isn the isn the acting acting is horrible is horrible and horrible and lacks and lacks enthusiasm lacks enthusiasm the enthusiasm the script script is boring is boring and and not even not even creative even creative they creative they choose they choose the choose the wrong the wrong characters wrong characters and characters and don and don even don even build even build on build on them on them just them just everything just everything put everything put together put together all together all the all the small the small parts small parts don parts don even don even add even add up add up to up to something to something great something great waste great waste of waste of time of time wouldn time wouldn classify wouldn classify this classify this as this as horror as horror though horror though it though it has it has elements has elements of elements of horror of horror they horror they ultimately they ultimately fail ultimately fail at fail at what at what they what they try they try to try to succeed to succeed it succeed it felt it felt more felt more like more like beginners like beginners short beginners short movie short movie than movie than by than by john by john carpenter john carpenter br carpenter br br br br sorry br sorry for sorry for my for my lack my lack of lack of information of information and information and detailed and detailed review detailed review just review just didn just didn have didn have the have the time the time to time to waste to waste to waste to write to write something write something exciting something exciting also exciting also sorry also sorry if sorry if my if my spelling my spelling and spelling and details and details are details are incorrect are incorrect couldn incorrect couldn really couldn really be really be bothered be bothered to bothered to research to research anything\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display a few classified reviews (adapted for bigram features)\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "predicted_labels = lr_model3.predict(sample_reviews)\n",
    "\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c11e0264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        21\n",
      "           1       1.00      0.84      0.91        19\n",
      "\n",
      "    accuracy                           0.93        40\n",
      "   macro avg       0.94      0.92      0.92        40\n",
      "weighted avg       0.93      0.93      0.92        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####Baseline 2: Random Forest\n",
    "#Unigram\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "num_synthetic_reviews = 100\n",
    "synthetic_reviews_unigram = [generate_synthetic_review_unigram(unigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "\n",
    "# Prepare actual reviews\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews_unigram + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews_unigram) + [1] * len(actual_reviews)\n",
    "\n",
    "# Create DataFrame for combined data\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "# Feature extraction using Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_df['Review'])\n",
    "y = data_df['Label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model1 = RandomForestClassifier()\n",
    "rf_model1.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model1.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0807465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: add and any are as been chance characters complete concerning explanation finds get growls is less locations looks million missed nerves of out over part real roles seemed she so spice stars thames that the there this to two useless was when word worse\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: 10 30 about actors actresses always and as at back be berkeley best better blondell bought branches bread busby by cagney cast cents choreographer conditions day did during economic end enthusiasm fabulous feel for forget frank gloomy great guy hollywood how in it its itself james joan keeler kibbee knowing little ll loaf luxury make many marching mchugh military millions minor movie movies must note of patriotism people powell precision remember respected ruby scraped see similar so studio system testimony that the then this time to together too type usa use viewed was were william with you\n",
      "Actual Label: Actual\n",
      "Predicted Label: Actual\n",
      "--------------------------------------------------\n",
      "Review: actually and annoying antithis any at be begin best both bring ella exist faces film funny gang have horses im included it its life made man of on opening period solved something sorry stuff that the this to today unfold use was worse wow yes\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: being big bunch calls certainly couldnt dance gang he it its leaden makes maybe moments much must of out perform primitivism role rubbish seem seen sees story sure the theres this to together true type understand wellplayed were white who worth zanes\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: at bad because being best bonus can cattle dialog doesnt driven enemies episodes for from had her in it midgets moneyoriented ms next of out pseudointellectual roguish salvage suffering than the think to utter very video wendell yelchin you\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random indices for selecting sample reviews\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "\n",
    "# Extract sample reviews and their actual labels\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "\n",
    "# Predict the labels for these reviews\n",
    "predicted_labels = rf_model1.predict(sample_reviews)\n",
    "\n",
    "# Display the results\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "\n",
    "    # Transforming the review features back to text (may not perfectly reconstruct the original text)\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    \n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27f35a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79        21\n",
      "           1       1.00      0.42      0.59        19\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.83      0.71      0.69        40\n",
      "weighted avg       0.82      0.72      0.70        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bigram\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to generate synthetic review using bigram model\n",
    "def generate_synthetic_review_bigram(bigram_counts, num_words=50):\n",
    "    if not bigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    current_word = random.choice(list(bigram_counts.keys()))\n",
    "    review = [current_word]\n",
    "\n",
    "    for _ in range(num_words - 1):\n",
    "        next_words = list(bigram_counts[current_word].keys())\n",
    "        next_word_weights = list(bigram_counts[current_word].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using bigram model\n",
    "num_synthetic_reviews = 100\n",
    "synthetic_reviews_bigram = [generate_synthetic_review_bigram(bigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "\n",
    "# Prepare actual reviews\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews_bigram + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews_bigram) + [1] * len(actual_reviews)\n",
    "\n",
    "# Create DataFrame for combined data\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "# Feature extraction using Bag-of-Words with bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(data_df['Review'])\n",
    "y = data_df['Label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model for bigram reviews\n",
    "rf_model2 = RandomForestClassifier()\n",
    "rf_model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model2.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ece2fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: all the also knows and grumpy and is audiences will bad movie be disappointed believe he but it certainly have disappointed and grew certainly grumpy woman have the he grew himself and his problem hysterical its is also is using it self it would its possibly knows all movie but movie was movie when of it possibly result problem of result is sacrificing himself self sacrificing the bad the worst think that this movie to believe turtle when using his was hysterical when audiences when this will be woman to worst movie would think\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: 1993 shine all characters and direction and have and the and very approach bit aussie blend bit self blend of br br br gosh characters was cinematography and crowe talent depreciating and developing his didn russell direction pace even in felt this film throughout gosh didn have yet his relationship impressed with impressive br in 1993 in developing just right lillie such loved the nearly all of nearly of softly pace was persistent really portrayals of really loved relationship with right and russell crowe russell talent see gladiator self depreciating shine and softly approach softly softly such typical talent even talent in the cinematography the portrayals this film throughout waas to see typical aussie very persistent waas impressed was impressive was just with lillie with russell yet to\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: acting was and published be re ever saw film if for sale if ever it it it needs more people needs to one more people should published for re released released and saw one see it should see sleeper film superb sleeper the acting to be was superb\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: 1972 short 20 hussein 2002 which abandoned subconscious able to about the absolute favorite after dark again ll alejandro jodorowsky an eyeball and an and at and cannibal and david and he and its and jame and mitch and one and ouch and proceed and sodomize and sound and throw and unsettling and you apart by are obviously argento dusan as alejandro as man ascension 2002 at times avant gard be able be compared be one been more begotten and better film bleed when bloody holes borders on bother me br br br the brain and branch poor branches bleed branches the bread and broken watch broughton 1972 business man but its but there but with by fishhooks by luico by what can be cannibal holocaust cannibalistically eat cerda after christ figure close my co write come from communion bread compared to cruelty has cruelty is cruelty this cut open dario argento dark horror david lynch davis are deals with designs repulsive did this didn bother direct the directors and directors such disturbed in disturbing and disturbing films divided into dreamwood in dusan makavejev early 20 earth men eat him element to elements of eraserhead but ever seen ever watch everyday human extreme elements extreme that eye is eyeball is eyes or fascinating and fast forward favorite it fellatio to figure and film again film and film by film dreamwood film eraserhead film invades film is film made film scores films ve first part fishhooks yuck flair can for br for nacho forward karim from the frued when fulci and functions then gard directors get in gets her gets his girls masturbate gives fellatio graphic violence ground girls guess what guy the hardcore pornography has to have close have sex he co her stomach him like him the him with his pregnant his privates holes in holocaust didn holocaust ve holy mountain horror as horror masterpiece how to human larvae human minds hump bloody hussein went hussien and if ever images that in for in horror in my in the in there in this in touch incest it influenced ascension into parts invades your is divided is fascinating is kind is much is my is pulled is real is the it borders it deals it reminds its functions its visual jame broughton jodorowsky dario jodorowsky the journey with karim hussien kind of knife sticking know how larvae is last part life by like communion like the ll have luico fulci lynch take made me makavejev and man gets man gives man sexual masterpiece the masturbate with me of me that me think men hump might be mind with minds well mitch davis more disturbed most disturbing most extreme most unsettling mountain sweet movie and much better much but my absolute my eyes my life nacho cerda narrator tells nature br need him never been nude woman obsession with obviously very of begotten of everyday of film of jodorowsky of like of the of those of woman on christ on hardcore on to one of open and or slightly ouch two out of out the ovarian eye part human part is part of part the part was parts of parts the people have people really poor guy pornography this pregnant sister privates pulled proceed to project in psychological element pulled apart pulled out re in real short really know realm of reminds me repulsive at salo and scores and screenplay for second part seen salo segment people sex with sexual obsession shocking taboos short film short narrator sister where slightly fast so extreme sodomize him sort of sound designs sticking out stomach cut strange psychological subconscious cruelty subconscious mind such as surrealist visuals sweet movie taboos surrealist take the talented to tarkovsky influenced tells us that come that if that much the abandoned the brain the branches the earth the film the first the graphic the ground the holy the last the most the ovarian the parts the screenplay the second the tarkovsky the the the third the unthinkable then nude there early there strange these people they did think of think they third part this film this part this project this segment those directors throw in times it times yes to avant to be to cannibalistically to direct to get to guess to knife to subconscious to think touch with tree branch tree branches two women unsettling film unsettling journey unthinkable realm urinate on us about vagina these ve ever ve never very talented violence of visual flair visuals and was so watch in watch the well sort went on what witnessed what you when broken when you where frued which is with his with images with incest with man with nature with shocking with the with tree witnessed business woman gets woman vagina women urinate write the yes but you might you need you re your subconscious yuck and\n",
      "Actual Label: Actual\n",
      "Predicted Label: Actual\n",
      "--------------------------------------------------\n",
      "Review: almost dont and bullets and hell and hightailed and poetry and wish ask this bullets and compare when consider tamales cultured and dont quite fire ask for reunion get to glenn and governments heads heads and hell is high school hightailed it in high is cultured is the it york like to main review poetry and possibly the quite possibly reunion scene review is scene when school fire sheriffand glenn shines almost story like tamales shines the main the us this story to compare to consider us governments when bill when you wish for york in you get\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display a few classified reviews (adapted for bigram features)\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "predicted_labels = rf_model2.predict(sample_reviews)\n",
    "\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71adfd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72        21\n",
      "           1       1.00      0.16      0.27        19\n",
      "\n",
      "    accuracy                           0.60        40\n",
      "   macro avg       0.78      0.58      0.50        40\n",
      "weighted avg       0.77      0.60      0.51        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Function to generate synthetic review using trigram model\n",
    "def generate_synthetic_review_trigram(trigram_counts, num_words=50):\n",
    "    if not trigram_counts:\n",
    "        return \"\"\n",
    "    \n",
    "    start_bigram = random.choice(list(trigram_counts.keys()))\n",
    "    review = list(start_bigram)\n",
    "\n",
    "    for _ in range(num_words - 2):\n",
    "        next_words = list(trigram_counts[start_bigram].keys())\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word_weights = list(trigram_counts[start_bigram].values())\n",
    "        next_word = random.choices(next_words, weights=next_word_weights)[0]\n",
    "        review.append(next_word)\n",
    "        start_bigram = (start_bigram[1], next_word)\n",
    "\n",
    "    return ' '.join(review)\n",
    "\n",
    "# Generate synthetic reviews using trigram model\n",
    "num_synthetic_reviews = 100\n",
    "synthetic_reviews_trigram = [generate_synthetic_review_trigram(trigram_counts) for _ in range(num_synthetic_reviews)]\n",
    "\n",
    "# Prepare actual reviews\n",
    "actual_reviews = reviews_df['Review'].sample(num_synthetic_reviews).tolist()\n",
    "\n",
    "# Combine and label data\n",
    "combined_reviews = synthetic_reviews_trigram + actual_reviews\n",
    "labels = [0] * len(synthetic_reviews_trigram) + [1] * len(actual_reviews)\n",
    "\n",
    "# Create DataFrame for combined data\n",
    "data_df = pd.DataFrame({'Review': combined_reviews, 'Label': labels})\n",
    "\n",
    "# Feature extraction using Bag-of-Words with trigrams\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "X = vectorizer.fit_transform(data_df['Review'])\n",
    "y = data_df['Label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model for trigram reviews\n",
    "rf_model3 = RandomForestClassifier()\n",
    "rf_model3.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = rf_model3.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "850ebcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: and im sorry and starting war but thats not complimentary laptop per down to the effects stock footage fg visual effects footage of them has lived her her whole life heres the plot horrorfan like me im sorry but in real horrorfan jack kennedy in kennedy in real laptop per student life fg visual like me or lived her whole maybe this movie me or has movie and im not complimentary laptop of them were or has lived per student heres plot slows down real horrorfan like rounded up and slows down to sorry but thats starting war maybe stock footage of student heres the thats not complimentary the plot slows them were rounded this movie and up and starting visual effects stock war maybe this were rounded up whole life fg\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: abduct people and an extremely small and gets no and well for bounty hunter james budget pictures fake cast and well commercial stills gallery decide to abduct deep secrets they drive me mad enough the obituaries extremely small budget failed to hold fake minute syrup for the bounty gallery cast and gets no lines hold deep secrets hunter james russo instead of relying james russo was just left on left on that lines instead of mad enough the me mad enough minute syrup commercial no lines instead obituaries failed to of relying on on an extremely on that fateful people and gets pictures fake minute relying on an russo was just secrets they decide small budget pictures stills gallery cast syrup commercial stills the bounty hunter the obituaries failed they decide to to abduct people to hold deep was just left well for the\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: and her im and since were any honorable mention are sir laurence as inspiration and belated honeymoon by brilliantly it should but only as by the waltz dancers which are deserve any honorable does not deserve downward but only farther downward but goes to scarp her and her her im watching honeymoon by the honorable mention goes im watching dvd in the middle inspiration and since it should trend just watching the laurence olivier brilliantly mention goes to middle of town movie does not not deserve any of town with olivier brilliantly it only as inspiration scarp in the should trend farther since were just sir laurence olivier the middle of the movie does the waltz dancers to scarp in town with her trend farther downward waltz dancers which watching the movie were just watching which are sir with her and\n",
      "Actual Label: Synthetic\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: 5th if you according the official acting chops funny adrian paul does ahead with the all others seem and looked like anyway adrian paul are keeping track be made for be sure they but all others but it felt but this episode can be sure chance with repeat channel good effort chops funny enough compartment according the compete in the continuity compartment according credible performance but debut on the does credible performance dvd releases highlander effort made but enough is the enough this new episode fails to fails to compete felt and looked fi channel good film the 5th first of new for tv movie for tv or funny enough is gets released on good effort made have chance with highlander but this highlander film the highlander the source if they move if you are in the continuity is the first is the vague it felt and it gets released it television debut keeping track made lack to acting like made for looked like made made but it made for tv made it television may have chance move ahead with movie anyway adrian new highlander film new trilogy well of new trilogy official sourse website on the sci or straight to original highlander but other two you others seem to paul does credible performance but all references to the released on dvd releases highlander the repeat viewings when sadly enough this sci fi channel seem to lack source may have sourse website this straight to dvd sure they will television debut on the 5th if the continuity compartment the first of the official sourse the original highlander the other two the sci fi the source may the vague references they move ahead they will be this episode fails this is the this new highlander to acting chops to compete in to dvd releases to lack to to the original track made it trilogy well if tv movie anyway tv or straight two you can vague references to viewings when it website this is well if they when it gets will be made with repeat viewings with the other you are keeping you can be\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n",
      "Review: action and you after taking few all the visual amateur softcore but an amateur softcore an impressive performance and call it and family of and production value and you got are not it at the end back this is be found not because you can boobs in there br br just br br know br br now br br trouble br br was br just bubbling br know this br now put br trouble swallowing br was very brandon howe here breast to be bubbling over with build up to but he manages but only after but that doesn but there not but unfortunately the call it potboiler can doesn mean can just come can see in classes in story come out of corrigan needs to couple pairs of crap this film crazy twist at deliver an impressive deserve pat on dialog is pretty disappointed in this doesn mean the doesn mean you end of movie example of just face that he family of the few more classes field with crazy film is offering film it has film was shot filmmaker br br filmmakers deserve pat for practically no found not one friends and family girl action and girl on girl good but unfortunately got yourselves movie gotta build up grace is brandon guy br br has all the having br br he having br he manages to here guy br his face that howe here guy impressive performance br in his face in story structure in there maybe in this film is brandon howe is offering but is pretty good is prime example it are not it has all it painful to it potboiler you it the dialog just because you just bubbling over just come out know this film left field with manages to deliver maybe some girl mean the filmmakers mean you should money but that more classes in movie and call movie my friends needs to stick no money but not it painful not one breast not one corrigan now put couple of an amateur of boobs in of just because of left field of movie and of the filmmaker of these performances offering but he on girl action on the back one breast to one corrigan needs one saving grace only after taking out of left over with potential painful to watch pairs of boobs pat on the people reciting it performance br br performances the one potboiler you gotta potential you can practically no money pretty good but prime example of probably friends and production value of put couple pairs re probably friends reciting it are saving grace is see in his shot for practically should br br softcore but there some girl on some of these stick to writing story structure you structure you can style and production swallowing the crap taking few more that doesn mean that he having the back this the crap this the dialog is the end of the filmmaker br the filmmakers deserve the one saving the people reciting the visual style there maybe some there not one these performances the they re probably this film is this film it this film was this is prime to be found to deliver an to it the to stick to to watch some to writing but trouble swallowing the twist at the unfortunately the people up to it value of an very disappointed in visual style and was shot for was very disappointed watch some of with crazy twist with potential you writing but only you can doesn you can just you can see you got yourselves you gotta build you should br yourselves movie my\n",
      "Actual Label: Actual\n",
      "Predicted Label: Synthetic\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display a few classified reviews (adapted for bigram features)\n",
    "random_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "sample_reviews = X_test[random_indices]\n",
    "actual_labels_sample = y_test.iloc[random_indices]\n",
    "predicted_labels = rf_model3.predict(sample_reviews)\n",
    "\n",
    "for i, index in enumerate(random_indices):\n",
    "    review_features = sample_reviews[i]\n",
    "    actual_label = actual_labels_sample.iloc[i]\n",
    "    predicted_label = predicted_labels[i]\n",
    "    review_text = ' '.join(vectorizer.inverse_transform(review_features)[0])\n",
    "    print(\"Review:\", review_text)\n",
    "    print(\"Actual Label:\", \"Actual\" if actual_label == 1 else \"Synthetic\")\n",
    "    print(\"Predicted Label:\", \"Actual\" if predicted_label == 1 else \"Synthetic\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
