{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71dd3d5",
      "metadata": {
        "id": "a71dd3d5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.util import ngrams\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3300a39c",
      "metadata": {
        "id": "3300a39c"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhFR4QPTM7BN",
        "outputId": "29fecf26-a125-42a2-ce77-5ba48b0aac0a"
      },
      "id": "hhFR4QPTM7BN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1UukubDNOI_",
        "outputId": "baf82821-3b2a-4d9e-ac58-8dcc03700e3f"
      },
      "id": "Y1UukubDNOI_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea7b0c3",
      "metadata": {
        "id": "cea7b0c3"
      },
      "outputs": [],
      "source": [
        "#Import GPT2's tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "GPT2model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0206beae",
      "metadata": {
        "id": "0206beae"
      },
      "outputs": [],
      "source": [
        "#Test out model and tokenizer with a prompting example\n",
        "sequence = \"write a movie review:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615a36a4",
      "metadata": {
        "id": "615a36a4"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(sequence, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a701ee8",
      "metadata": {
        "id": "1a701ee8"
      },
      "outputs": [],
      "source": [
        "attention_mask = torch.ones(inputs.shape, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb1629e",
      "metadata": {
        "id": "dbb1629e"
      },
      "outputs": [],
      "source": [
        "outputs = GPT2model.generate(\n",
        "    inputs,\n",
        "    attention_mask=attention_mask,  # Add the attention mask here\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=100,\n",
        "    pad_token_id=tokenizer.pad_token_id  # Explicitly set the pad token ID\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46a8fb6",
      "metadata": {
        "id": "e46a8fb6"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e44194",
      "metadata": {
        "id": "b2e44194",
        "outputId": "1c8e5d2d-02c9-4dd7-9a1f-ce03bea6698c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'write a movie review:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "generated_text\n",
        "\n",
        "#Non-sensical due to no-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f627862",
      "metadata": {
        "id": "6f627862"
      },
      "outputs": [],
      "source": [
        "#Acquire the dataset from AllReviews csv file and clean it up to a usable form\n",
        "AllReviews = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AllReviews.csv')\n",
        "AllReviews = AllReviews.loc[(AllReviews[\"titleType\"] == \"tvMovie\")|(AllReviews[\"titleType\"] == \"movie\")]\n",
        "\n",
        "# Filter relevant columns and remove duplicates\n",
        "Reviews_Genres_Title = AllReviews[[\"Review\", \"genres\"]].drop_duplicates()\n",
        "# Remove rows with missing reviews\n",
        "Reviews_Genres_Title = Reviews_Genres_Title.dropna(subset=[\"Review\"])\n",
        "# Remove rows with undefined genres\n",
        "Reviews_Genres_Title = Reviews_Genres_Title[Reviews_Genres_Title[\"genres\"] != '\\\\N']\n",
        "\n",
        "# Copy for further processing\n",
        "Complete_Reviews = Reviews_Genres_Title.copy()\n",
        "# Convert reviews to lowercase\n",
        "Complete_Reviews['Review'] = Complete_Reviews['Review'].str.lower()\n",
        "# Remove HTML tags\n",
        "Complete_Reviews['Review'] = Complete_Reviews['Review'].str.replace('<[^>]+>', '', regex=True)\n",
        "# Remove non-alphanumeric characters\n",
        "Complete_Reviews['Review'] = Complete_Reviews['Review'].str.replace('[^A-Za-z0-9 ]+', '', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce16321",
      "metadata": {
        "id": "5ce16321"
      },
      "outputs": [],
      "source": [
        "#Breaking up review genres\n",
        "Complete_Reviews['genres']=Complete_Reviews['genres'].str.replace(',',' ')\n",
        "Complete_Reviews['genres']=Complete_Reviews['genres'].str.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54181f38",
      "metadata": {
        "id": "54181f38",
        "outputId": "c8eb0141-227c-4eda-9334-8a8064f23a00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29    [Action, Adventure, Biography]\n",
              "30    [Action, Adventure, Biography]\n",
              "31    [Action, Adventure, Biography]\n",
              "32    [Action, Adventure, Biography]\n",
              "63                           [Drama]\n",
              "64                           [Drama]\n",
              "65                           [Drama]\n",
              "66                           [Drama]\n",
              "67                           [Drama]\n",
              "71          [Drama, Fantasy, Horror]\n",
              "Name: genres, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#Currently genre is not used in the review generation or training\n",
        "Complete_Reviews['genres'][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024ae830-d6af-4517-b51c-de53ec8ab345",
      "metadata": {
        "id": "024ae830-d6af-4517-b51c-de53ec8ab345",
        "outputId": "b0186880-2a4a-488f-c8a9-0edf30a15a2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "torch.cuda.is_available()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36733d5",
      "metadata": {
        "id": "b36733d5",
        "outputId": "8312ff24-ca2a-42c8-f628-e453786d612e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334744576"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "GPT2model.to(device)\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f47aba",
      "metadata": {
        "id": "76f47aba"
      },
      "outputs": [],
      "source": [
        "#Get data into the form of a list to feed to the encoder\n",
        "text = Complete_Reviews['Review'].to_numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e3c63b",
      "metadata": {
        "id": "65e3c63b",
        "outputId": "5917ade0-613c-429c-f880-2b04cac9784e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the story of the kelly gang is believed to be the worlds first feature length film running at between 65 and 70 minutes it was billed at the time as the longest film ever made it toured australia for nine years and was an enormous successtoday only fragments survive and it is hard to judge the films artistic merits about nine minutes of footage exists  some found on a garbage dump in melbourne some of this footage may be outtakes the footage is held by screensound australia the national screen and sound archive in canberrathe sequences show some enthusiastic acting although the camerawork is static like most films of the period the most remarkable shot is probably when a priest carrying a wounded man over his shoulder walks toward and just past the camera creating a strong sense of drama and movement the final shootout scene is also well filmed  with ned kelly moving and shooting toward the camera as troopers flee to the sidesa remarkable film of great historical importance that all film students should see up until world war 1 when initially neutral america began to dominate the world of film distribution australia had one of the most thriving and innovative film industries in the world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "text[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89ca100",
      "metadata": {
        "id": "d89ca100",
        "outputId": "94445ce8-0e87-4ae4-ab00-99cb2b489440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79709"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0128437e",
      "metadata": {
        "id": "0128437e"
      },
      "outputs": [],
      "source": [
        "test_text = text[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44564f3a",
      "metadata": {
        "id": "44564f3a",
        "outputId": "4afc3b9a-ea7c-4665-8e82-d2a6fbc59ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecfdc9be",
      "metadata": {
        "id": "ecfdc9be"
      },
      "outputs": [],
      "source": [
        "# Setting up training with truncated reviews\n",
        "input_ids = []\n",
        "max_length = 256  # GPT-2's maximum sequence length\n",
        "\n",
        "for review in text:\n",
        "    encoded_review = tokenizer.encode(review, max_length=max_length, truncation=True, return_tensors='pt', padding = 'max_length')\n",
        "    input_ids.append(encoded_review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a6a1a5",
      "metadata": {
        "id": "f0a6a1a5"
      },
      "outputs": [],
      "source": [
        "#Setting up training with 79709 reviews\n",
        "#max_length = 256\n",
        "#input_ids = []\n",
        "#masks = []\n",
        "##Iterate the encoder over the entire length of the text\n",
        "#for i in range(len(text)):\n",
        "#    enc = tokenizer(text[i], return_tensors='pt', max_length=max_length, truncation=True, padding = 'max_length')\n",
        "#\n",
        "#    input_ids.append(enc['input_ids'])\n",
        "#    masks.append(enc['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#input_ids = tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True).to(device)"
      ],
      "metadata": {
        "id": "JnvMolgqvxRe"
      },
      "id": "JnvMolgqvxRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVqQEf6Px_u6",
        "outputId": "b1064ab9-1d1d-4eba-b94b-8e536e9906e2"
      },
      "id": "aVqQEf6Px_u6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1169,  1621,   286,   262,   885, 12810,  7706, 40538,  6194,  4340,\n",
              "          1111,   262,  4082,   286,   262, 38132,  1373,   666,  2646,  2831,\n",
              "           290,   262, 22106,   286,   281, 38132,  1373,   666,  5369,   772,\n",
              "           517,  5566,   340, 28783,    82,   262, 22106,   286,   262,  3895,\n",
              "          2646,  1296,  1078,   258, 11621,   717,  3895, 13664,  3807,   373,\n",
              "          7924,   416,  1149,   829,   256,  4548,   290, 18976,   379,   262,\n",
              "           256,  4548,  1641,    82,  1149,  1010,  4244,  7964,   287,   262,\n",
              "          7758, 12544, 23200,   286,   339,  5943,  3900,  6198,   612,   547,\n",
              "           645,   987,    83, 30540, 45127,   373,  6157,   416,   281, 38500,\n",
              "         40228,   508,   635,  2810,  2128,  3048,  1390, 29276,   290,  8169,\n",
              "          1659,  1350,  1381,   340,  1575,  8576,   284,   787,   475,   326,\n",
              "          1637,   290,   517,   373, 11911,  1626,   663,   717,  1285,   286,\n",
              "         14135,   340, 44119,   287,  7758, 12544,   319, 21576,  1110, 40538,\n",
              "           290,   373,  1568,  3402,  1973, 38132,  1373,   544,   287,   649,\n",
              "         27689,   392,   290,   287,   275,   799,   391,  8807, 21441,   286,\n",
              "           262,  2656,  3227,   286,   517,   621,   530,  1711,   389,  1900,\n",
              "           284,  2152,   290,   389, 17232,   379,   262,  2260,  2646,   290,\n",
              "          2128, 15424,   460, 31358,   981,   617,   286,   262,  9640,   318,\n",
              "          2048, 37293,   584, 17894,   389, 15052, 26987,   262,  8564, 13759,\n",
              "          4873,  4283,   319,   543,   262,  2646,   373,  2823, 44525,  2952,\n",
              "           287,  6143,   523,   355,   356,  2342,   299,   276,   787,   465,\n",
              "          2457,  1302,  1028,   262,  1644,   379,  1278,   268,   808,   272,\n",
              "           287,   465, 13273,  6050,   286,  8328,   339, 44370,   290, 17488,\n",
              "            82,   287,   881,   262,   976,  5642,   355,   257,  7019,   358,\n",
              "           323,  4875,  1245, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "         50256, 50256, 50256, 50256, 50256, 50256]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2534549d",
      "metadata": {
        "id": "2534549d",
        "outputId": "1b27b6fd-09fc-48bb-a569-7a97551e7c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the story of the kelly gang 1906 symbolizes both the birth of the australian film industry and the emergence of an australian identity even more significantly it heralds the emergence of the feature film formatthe worlds first featurelength movie was directed by charles tait and filmed at the tait familys chartersville estate in the melbourne suburb of heidelberg originally there were no intertitles narration was performed by an onstage lecturer who also provided sound effects including gunfire and hoofbeats it cost 1000 to make but that money and more was recovered within its first week of screening it premiered in melbourne on boxing day 1906 and was later shown across australia in new zealand and in britainonly fragments of the original production of more than one hour are known to exist and are preserved at the national film and sound archive canberra while some of the footage is almost pristine other segments are severely distorted the sensitive nitrate stock on which the film was shot deteriorated quickly in storage so as we watch ned make his final stand against the police at glenrowan in his legendary suit of armor he bends and morphs in much the same manner as a modernday digital effect'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Visual of how to decode first element of encoded input ids\n",
        "test_output = tokenizer.decode(input_ids[0][0], skip_special_tokens=True)\n",
        "test_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a3548d-a821-4e89-8fae-afbd3b951373",
      "metadata": {
        "id": "d2a3548d-a821-4e89-8fae-afbd3b951373",
        "outputId": "f1989de9-1d9a-42c7-c623-e99881e335c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334744576"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25660d44",
      "metadata": {
        "id": "25660d44"
      },
      "outputs": [],
      "source": [
        "# set training parameters\n",
        "train_batch_size = 20\n",
        "num_train_epochs = 5\n",
        "learning_rate = 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b476ddc9",
      "metadata": {
        "id": "b476ddc9",
        "outputId": "99530d81-0a2e-45af-fe6b-70aa045ef3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334744576"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Ensure all tensors are 1D\n",
        "input_ids_1d = [t.flatten() for t in input_ids]\n",
        "#masks_1d = [t.flatten() for t in masks]\n",
        "\n",
        "# Now pad the sequence\n",
        "#padded_input_ids = pad_sequence(input_ids_1d, batch_first=True)\n",
        "#padded_masks = pad_sequence(masks_1d, batch_first=True)\n",
        "padded_input_ids = torch.stack(input_ids_1d)\n",
        "\n",
        "# Create a dataset and dataloader\n",
        "#dataset = TensorDataset(padded_input_ids, padded_masks)\n",
        "#data_loader = DataLoader(dataset, batch_size=train_batch_size, num_workers=4)\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95642a4",
      "metadata": {
        "id": "b95642a4",
        "outputId": "2319a652-360f-4a82-bbb1-44f8ae692083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "334744576"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# initialize optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(GPT2model.parameters(), lr=learning_rate)\n",
        "total_steps = len(input_ids) * num_train_epochs // train_batch_size\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67c91f8f",
      "metadata": {
        "id": "67c91f8f",
        "outputId": "f3d70d7a-9605-4cf2-84b5-07136a688c99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 79709\n",
            "5000 / 79709\n",
            "10000 / 79709\n",
            "15000 / 79709\n",
            "20000 / 79709\n",
            "25000 / 79709\n",
            "30000 / 79709\n",
            "35000 / 79709\n",
            "40000 / 79709\n",
            "45000 / 79709\n",
            "50000 / 79709\n",
            "55000 / 79709\n",
            "60000 / 79709\n",
            "65000 / 79709\n",
            "70000 / 79709\n",
            "75000 / 79709\n",
            "Epoch: 1, Loss: 0.3107\n",
            "0 / 79709\n",
            "5000 / 79709\n",
            "10000 / 79709\n",
            "15000 / 79709\n",
            "20000 / 79709\n",
            "25000 / 79709\n",
            "30000 / 79709\n",
            "35000 / 79709\n",
            "40000 / 79709\n",
            "45000 / 79709\n",
            "50000 / 79709\n",
            "55000 / 79709\n",
            "60000 / 79709\n",
            "65000 / 79709\n",
            "70000 / 79709\n",
            "75000 / 79709\n",
            "Epoch: 2, Loss: 0.2998\n",
            "0 / 79709\n",
            "5000 / 79709\n",
            "10000 / 79709\n",
            "15000 / 79709\n",
            "20000 / 79709\n",
            "25000 / 79709\n",
            "30000 / 79709\n",
            "35000 / 79709\n",
            "40000 / 79709\n",
            "45000 / 79709\n",
            "50000 / 79709\n",
            "55000 / 79709\n",
            "60000 / 79709\n",
            "65000 / 79709\n",
            "70000 / 79709\n",
            "75000 / 79709\n",
            "Epoch: 3, Loss: 0.2956\n",
            "0 / 79709\n",
            "5000 / 79709\n",
            "10000 / 79709\n",
            "15000 / 79709\n",
            "20000 / 79709\n",
            "25000 / 79709\n",
            "30000 / 79709\n",
            "35000 / 79709\n",
            "40000 / 79709\n",
            "45000 / 79709\n",
            "50000 / 79709\n",
            "55000 / 79709\n",
            "60000 / 79709\n",
            "65000 / 79709\n",
            "70000 / 79709\n",
            "75000 / 79709\n",
            "Epoch: 4, Loss: 0.2932\n",
            "0 / 79709\n",
            "5000 / 79709\n",
            "10000 / 79709\n",
            "15000 / 79709\n",
            "20000 / 79709\n",
            "25000 / 79709\n",
            "30000 / 79709\n",
            "35000 / 79709\n",
            "40000 / 79709\n",
            "45000 / 79709\n",
            "50000 / 79709\n",
            "55000 / 79709\n",
            "60000 / 79709\n",
            "65000 / 79709\n",
            "70000 / 79709\n",
            "75000 / 79709\n",
            "Epoch: 5, Loss: 0.2919\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "GPT2model.train()\n",
        "for epoch in range(num_train_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for i in range(0, len(input_ids)-1, train_batch_size):\n",
        "    #for input, mask in data_loader:\n",
        "        # slice the input ids tensor to get the current batch\n",
        "        batch_input_ids = padded_input_ids[i:i+train_batch_size]\n",
        "        #batch_masks = padded_masks[i:i+train_batch_size].cuda()\n",
        "        batch_labels = batch_input_ids.clone()\n",
        "        batch_labels[:, :-1] = batch_labels[:, 1:].clone()\n",
        "        # set label ids to -100 for padded tokens\n",
        "        batch_labels[batch_labels == tokenizer.pad_token_id] = -100\n",
        "        ## create shifted labels for each input in the batch\n",
        "        #batch_labels = batch_input_ids.clone()\n",
        "        #batch_labels[:, :-1] = batch_labels[:, 1:].clone()\n",
        "        # set label ids to -100 for padded tokens\n",
        "        #batch_labels[batch_labels == tokenizer.pad_token_id] = -100\n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass\n",
        "        #outputs = GPT2model(input_ids=batch_input_ids, attention_mask = batch_masks, labels=batch_labels)\n",
        "        outputs = GPT2model(input_ids=batch_input_ids.cuda(), labels=batch_labels.cuda())\n",
        "        loss = outputs[0]\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "        # clip gradients to prevent exploding gradients problem\n",
        "        torch.nn.utils.clip_grad_norm_(GPT2model.parameters(), 1.0)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        if i % 5000 == 0:\n",
        "            print(i, '/', len(input_ids))\n",
        "    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, epoch_loss/len(input_ids)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9a373e",
      "metadata": {
        "id": "6e9a373e",
        "outputId": "e1dd9743-7594-4627-e6d9-b34de07218ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./results/tokenizer_config.json',\n",
              " './results/special_tokens_map.json',\n",
              " './results/vocab.json',\n",
              " './results/merges.txt',\n",
              " './results/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# save the trained model\n",
        "output_dir = './results/'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "GPT2model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('./results/')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "GPT2model = GPT2LMHeadModel.from_pretrained('./results/')\n",
        "max_length = 256"
      ],
      "metadata": {
        "id": "PMPIJvr3tYy7"
      },
      "id": "PMPIJvr3tYy7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ee160f-20d5-4cbe-a86e-075f917563c5",
      "metadata": {
        "id": "82ee160f-20d5-4cbe-a86e-075f917563c5",
        "outputId": "2aaedc00-df63-463a-d5ea-04cd5ab56f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1930968576"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221e399b",
      "metadata": {
        "id": "221e399b"
      },
      "outputs": [],
      "source": [
        "#Try the same prompt with the newly fine tuned model\n",
        "sequence = [\"write a movie review:\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8bc3e1d",
      "metadata": {
        "id": "c8bc3e1d"
      },
      "outputs": [],
      "source": [
        "#Iterate the encoder over the entire length of the text\n",
        "seq_ids = tokenizer.encode(sequence, return_tensors='pt')\n",
        "\n",
        "attention_mask = torch.ones(seq_ids.shape, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6561306a",
      "metadata": {
        "id": "6561306a",
        "outputId": "9456f92b-6eb3-404a-c15e-9cadbc7cd39b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        }
      ],
      "source": [
        "#outputs = GPT2model.generate(seq_ids, max_length=max_length, do_sample=True, top_k=50, max_new_tokens = max_length)\n",
        "outputs = outputs = GPT2model.generate(\n",
        "    seq_ids,\n",
        "    attention_mask=attention_mask,  # Add the attention mask here\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=100,\n",
        "    pad_token_id=tokenizer.pad_token_id  # Explicitly set the pad token ID\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba084ee4",
      "metadata": {
        "id": "ba084ee4"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb95d4cb",
      "metadata": {
        "id": "cb95d4cb",
        "outputId": "9c7fb906-aa77-4c52-c9e7-b4dab37144e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' a story an story i say so you tell and it be you the is to seen see if are characters that make movie well a which to who the is are a matter fact the is of in one in to world is and to the part which'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "generated_text"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}